from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage
from nexus_agent.core.state import AgentState
from nexus_agent.tools import tools

wiki_func = tools[0]
llm = ChatOllama(model="llama3.2", temperature=0)

# FEW-SHOT PROMPT: Ã–rnekli anlatÄ±m
system_prompt = """You are a Research Assistant.
You MUST search Wikipedia for general knowledge questions.

EXAMPLES:
User: "AtatÃ¼rk kimdir?"
You: SEARCH: AtatÃ¼rk

User: "Python nedir?"
You: SEARCH: Python programming language

User: "Merhaba"
You: Merhaba! Size nasÄ±l yardÄ±mcÄ± olabilirim?

INSTRUCTION:
If the user asks a question, output "SEARCH: <query>".
If the user greets, just reply.
"""

def researcher_node(state: AgentState):
    messages = [SystemMessage(content=system_prompt)] + state["messages"]
    
    response = llm.invoke(messages)
    content = response.content.strip()
    
    print(f"   ğŸ‘€ [DEBUG RESEARCHER]: '{content}'")
    
    # "SEARCH:" veya "Search:" yakala
    if "SEARCH:" in content.upper():
        # Temizlik
        if "SEARCH:" in content: 
            query = content.split("SEARCH:")[1].strip()
        elif "Search:" in content: 
            query = content.split("Search:")[1].strip()
        else: 
            query = content # Fallback
        
        print(f"   ğŸŒ (Wiki) AranÄ±yor: '{query}'")
        
        try:
            tool_result = wiki_func.invoke(query)
        except Exception as e:
            tool_result = str(e)
            
        final_prompt = f"Search Result: {tool_result}\n\nBased on this result, answer the user's question in Turkish."
        messages.append(HumanMessage(content=final_prompt))
        
        # Final cevabÄ± Ã¼ret
        final_response = llm.invoke(messages)
        return {"messages": [final_response]}
    
    return {"messages": [response]}